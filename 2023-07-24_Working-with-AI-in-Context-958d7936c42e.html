<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Working with AI in Context</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Working with AI in Context</h1>
</header>
<section data-field="subtitle" class="p-summary">
What is positional encoding and why do we need it?
</section>
<section data-field="body" class="e-content">
<section name="deeb" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><p name="8dea" id="8dea" class="graf graf--p graf--leading">Working with AI in Context</p><p name="f96f" id="f96f" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What is positional encoding and why do we need it?</strong></p><p name="135a" id="135a" class="graf graf--p graf-after--p">Positional encoding is a way of adding information about the order and position of words or tokens in a sequence to a language model, especially a transformer model that does not use recurrence or convolution. Positional encoding can be done by adding or multiplying a vector of values to each token’s embedding vector. Different positional encoding schemes use different functions to generate these values, such as sinusoidal functions, learned embeddings, relative positions, etc.</p><p name="6aac" id="6aac" class="graf graf--p graf-after--p">Positional encoding is important because it helps the model to capture the sequential and contextual information of the input sequence, which can affect the meaning and interpretation of the output. For example, consider the sentence “I saw a man with a telescope.” Depending on the position of the words, this sentence can have different meanings:</p><ul class="postList"><li name="92f1" id="92f1" class="graf graf--li graf-after--p">I saw (a man with a telescope) -&gt; I saw a man who had a telescope.</li><li name="a834" id="a834" class="graf graf--li graf-after--li">(I saw a man) with a telescope -&gt; I used a telescope to see a man.</li></ul><p name="a0b8" id="a0b8" class="graf graf--p graf-after--li">Without positional encoding, the model might not be able to distinguish the semantic differences between these two meanings and produce an incorrect or ambiguous result.</p><p name="c733" id="c733" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What are Rope and Alibi and how do they relate to context windows in LLMs?</strong></p><p name="714b" id="714b" class="graf graf--p graf-after--p">Rope and Alibi are two types of positional encoding schemes that are designed to handle longer sequences and avoid the limitations of absolute position embeddings (APE). Rope stands for <strong class="markup--strong markup--p-strong">R</strong>elative <strong class="markup--strong markup--p-strong">P</strong>osition <strong class="markup--strong markup--p-strong">E</strong>mbeddings, and it uses relative distances between tokens instead of absolute positions to encode the order information. Alibi stands for <strong class="markup--strong markup--p-strong">A</strong>bsolute <strong class="markup--strong markup--p-strong">L</strong>inear <strong class="markup--strong markup--p-strong">I</strong>nput-<strong class="markup--strong markup--p-strong">B</strong>ased <strong class="markup--strong markup--p-strong">I</strong>ncremental, and it uses a linear function of the input token index to generate the positional values. Both Rope and Alibi can handle sequences up to 32,768 tokens long.<a href="https://www.lesswrong.com/posts/pHPmMGEMYefk9jLeh/llm-basics-embedding-spaces-transformer-token-vectors-are" data-href="https://www.lesswrong.com/posts/pHPmMGEMYefk9jLeh/llm-basics-embedding-spaces-transformer-token-vectors-are" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">(link)</a></p><p name="95f9" id="95f9" class="graf graf--p graf-after--p">Context windows are segments of the input sequence that are processed by the language model at each step. For example, if the input sequence has 100 tokens and the context window size is 10, then the model will process 10 tokens at a time, starting from the first 10 tokens, then shifting by one token and processing the next 10 tokens, and so on until it reaches the end of the sequence. Context windows are used to reduce the memory and computational requirements of processing long sequences, as well as to capture local dependencies within the sequence. However, context windows also introduce some challenges, such as how to handle information across different windows, how to deal with boundary effects, and how to ensure consistency and coherence in the output.<a href="https://wandb.ai/mostafaibrahim17/ml-articles/reports/An-Overview-of-Large-Language-Models-LLMs---VmlldzozODA3MzQz" data-href="https://wandb.ai/mostafaibrahim17/ml-articles/reports/An-Overview-of-Large-Language-Models-LLMs---VmlldzozODA3MzQz" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">(link)</a></p><p name="a00a" id="a00a" class="graf graf--p graf-after--p">Rope and Alibi are useful for dealing with context windows in LLMs because they can preserve the relative order information within and across windows without losing resolution or causing conflicts. For example, Rope can encode the relative distance between any two tokens in the same window or different windows by using an offset parameter that shifts the position index according to the window index. Alibi can encode the absolute position of any token in any window by using a linear function that scales with the window index. Both Rope and Alibi can also handle variable-length windows and dynamic shifts without requiring re-computation or re-alignment of the positional values.<a href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/" data-href="https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">(link)</a></p><p name="bc61" id="bc61" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">It’s getting real; pay attention.</strong></p><p name="e95e" id="e95e" class="graf graf--p graf-after--p">Generating semantic and contextual links between all the tokens in the user request and system response history is solved using an in-memory graph datastore which creates and maintains edges between all the tokens within the context window. The memory and processing required to perform this for even a few hundred tokens is enormous, which is where attention comes into play.</p><p name="1f24" id="1f24" class="graf graf--p graf-after--p">The graph of the tokens in the context window is a way of representing the input sequence as a set of nodes and edges, where each node is a token, and each edge is a connection between two tokens. The connection can be based on various criteria, such as the position, distance, similarity, or relevance of the tokens. For example, in a full self-attention mechanism, every token is connected to every other token in the sequence, forming a complete graph. In a sparse attention mechanism, only some tokens are connected to each other, forming a partial graph.</p><p name="828a" id="828a" class="graf graf--p graf-after--p">The graph size and memory footprint are measures of how much space and resources are needed to store and process the graph. The graph size depends on the number of nodes and edges in the graph, while the memory footprint depends on the size and type of the data associated with each node and edge. For example, in a full self-attention mechanism, the graph size and memory footprint are quadratic with respect to the input sequence length, because every token has an embedding vector and a similarity score with every other token. In a sparse attention mechanism, the graph size and memory footprint are linear or sublinear with respect to the input sequence length, because only some tokens have embedding vectors and similarity scores with each other.</p><p name="f8e6" id="f8e6" class="graf graf--p graf-after--p">Attention mechanisms work to reduce the graph size and memory footprint by sparsifying or compressing the graph, i.e., by reducing the number of nodes and edges or the amount of data associated with them. There are different ways to achieve this, such as:</p><ul class="postList"><li name="a6a5" id="a6a5" class="graf graf--li graf-after--p">Using structural information to limit the connections between tokens based on their position or relation in the sequence. For example, in local attention, only tokens within a certain window size are connected to each other. In global-local attention, some tokens are connected to all tokens (global), while others are connected to nearby tokens (local). In block attention, tokens are grouped into blocks and only blocks within a certain distance are connected to each other.</li><li name="5f67" id="5f67" class="graf graf--li graf-after--li">Using random sampling or hashing to select a subset of tokens or connections based on some probability distribution or function. For example, in random attention, each token is connected to a fixed number of randomly chosen tokens. In clustered attention, tokens are clustered into groups based on their similarity or relevance, and only tokens within the same group or adjacent groups are connected to each other. In hashed attention, tokens are mapped to buckets based on a hash function, and only tokens within the same bucket or neighboring buckets are connected to each other.</li><li name="e18a" id="e18a" class="graf graf--li graf-after--li">Using low-rank approximation or factorization to reduce the dimensionality or complexity of the data associated with each node or edge. For example, in low-rank attention, each token’s embedding vector is decomposed into two smaller vectors that are multiplied together to obtain the original vector. In linear attention, each token’s similarity score with another token is computed by using a linear function instead of a SoftMax<a href="https://deepai.org/machine-learning-glossary-and-terms/softmax-layer" data-href="https://deepai.org/machine-learning-glossary-and-terms/softmax-layer" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">(link)</a> function.</li></ul><p name="0d55" id="0d55" class="graf graf--p graf-after--li">some examples of the trade-offs between different types of attention mechanisms:</p><ul class="postList"><li name="341e" id="341e" class="graf graf--li graf-after--p">Full self-attention vs. sparse attention: Full self-attention connects every token to every other token in the sequence, forming a complete graph. This allows the model to capture global dependencies and long-range interactions, but it also increases the graph size and memory footprint quadratically with respect to the input sequence length. Sparse attention connects only some tokens to each other, forming a partial graph. This reduces the graph size and memory footprint linearly or sublinearly with respect to the input sequence length, but it also limits the model’s ability to capture global dependencies and long-range interactions.<a href="https://www.scaler.com/topics/deep-learning/attention-mechanism-deep-learning/" data-href="https://www.scaler.com/topics/deep-learning/attention-mechanism-deep-learning/" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">(link)</a></li><li name="ae38" id="ae38" class="graf graf--li graf-after--li">Absolute position embeddings vs. relative position embeddings: Absolute position embeddings encode the absolute position of each token in the sequence as a vector and add it to the token’s embedding vector. This allows the model to capture the order and position information of the tokens, but it also requires a fixed maximum sequence length and a large number of parameters. Relative position embeddings encode the relative distance between each pair of tokens in the sequence as a vector and add it to the similarity score between them. This allows the model to capture the order and position information of the tokens, but it also introduces some complexity and ambiguity in computing and interpreting the similarity scores.</li><li name="2b41" id="2b41" class="graf graf--li graf-after--li">Low-rank attention vs. linear attention: Low-rank attention reduces the dimensionality of each token’s embedding vector by decomposing it into two smaller vectors that are multiplied together to obtain the original vector. This reduces the amount of data associated with each node in the graph, but it also introduces some approximation error and distortion. Linear attention reduces the complexity of computing each token’s similarity score with another token by using a linear function instead of a softmax<a href="https://deepai.org/machine-learning-glossary-and-terms/softmax-layer" data-href="https://deepai.org/machine-learning-glossary-and-terms/softmax-layer" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">(link)</a> function. This reduces the amount of computation required for each edge in the graph, but it also introduces some approximation error and distortion.<a href="https://doi.org/10.3389/fncom.2020.00029" data-href="https://doi.org/10.3389/fncom.2020.00029" class="markup--anchor markup--li-anchor" rel="noopener" target="_blank">(link)</a></li></ul><p name="39f4" id="39f4" class="graf graf--p graf-after--li"><strong class="markup--strong markup--p-strong">Improving context size through attention; is it real or an illusion?</strong></p><p name="1239" id="1239" class="graf graf--p graf-after--p">Context windows seem to be continuously expanding; however, this is somewhat of an illusion as the memory requirements for generating a complete context graph is not reduced, the improvements come from altering the scope of the attention mechanism by removing graph edges between tokens that are distant from each other in the original text. This certainly helps reduce the memory footprint and complexity of the graph, but it comes at the cost of removing potentially critical semantic links between logically associated concepts that are temporally separated. So, as we can see the concept is a genuinely valuable enhancement to reduce memory requirements, but it comes at a cost.</p><p name="ed51" id="ed51" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">How can we actually expand context length while maintaining the full semantic web-of-context?</strong></p><p name="86cf" id="86cf" class="graf graf--p graf-after--p">One possible mechanism to factually expand context size is through quantization of the positional encodings which is an experimental technique that aims to reduce the memory and computational cost of positional encoding by using discrete values instead of continuous values. Quantization of positional encoding can be achieved by applying a quantization function to the positional encoding vectors, such as rounding, binning, or hashing. For example, one can round the positional values to the nearest integer, or use a hash function to map the positional values to a fixed number of buckets.<a href="https://arxiv.org/abs/2305.16843" data-href="https://arxiv.org/abs/2305.16843" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">(link)</a> <a href="https://arxiv.org/abs/2305.19466" data-href="https://arxiv.org/abs/2305.19466" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">(link)</a></p><p name="d898" id="d898" class="graf graf--p graf-after--p">Quantization of positional encoding can potentially improve the context length of LLMs by reducing the number of distinct values that need to be stored and processed. However, quantization of positional encoding also introduces some drawbacks, such as information loss, distortion, and collisions. Information loss occurs when the quantization function discards some details of the original positional values. Distortion occurs when the quantization function changes the relative distances or angles between different positional vectors. Collisions occur when the quantization function maps different positional vectors to the same value, causing ambiguity and confusion for the model.<a href="https://doi.org/10.48550/arXiv.2305.16843" data-href="https://doi.org/10.48550/arXiv.2305.16843" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">(link)</a></p><p name="3b4c" id="3b4c" class="graf graf--p graf-after--p">Therefore, quantization of positional encoding is a trade-off between efficiency and accuracy. Depending on the task and the model, quantization of positional encoding may or may not improve the performance of the model. Some empirical studies have shown that quantization of positional encoding can achieve comparable or even better results than continuous positional encoding on some tasks, such as machine translation and text summarization. However, more research is needed to understand the theoretical and practical implications of quantization of positional encoding.</p><p name="275c" id="275c" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What are vision transformers and how do they use positional encoding?</strong></p><p name="3867" id="3867" class="graf graf--p graf-after--p">Not everything AI is about generative AI, there are many areas where the fundamental needs overlap. One such area is in the use of AI Vision. Vision transformers are models that apply the transformer architecture to image recognition and generation tasks. Vision transformers treat an input image as a sequence of patches, similar to how transformers treat text as a sequence of words. Each patch is flattened into a vector and projected to a lower-dimensional embedding. To preserve the spatial information of the patches, positional embeddings are added to the patch embeddings. The resulting sequence of vectors is then fed to a standard transformer encoder, which uses self-attention to learn the relationships between different patches. The output of the encoder can be used for various downstream tasks, such as image classification, segmentation, or generation.</p><p name="c6ef" id="c6ef" class="graf graf--p graf-after--p">Vision transformers use positional encoding to provide spatial cues for the model to understand where each patch is located in the image grid. Without positional encoding, the model might not be able to distinguish between different orientations or layouts of the same image content. For example, consider an image of a cat sitting on a couch. Depending on where the cat is positioned in the image (top-left, center, bottom-right, etc.), the model might need to produce different outputs or predictions.</p><p name="6962" id="6962" class="graf graf--p graf-after--p">Vision transformers can use different types of positional encoding schemes for different tasks or datasets. For example, some vision transformers use learned absolute position embeddings (APE) for image classification tasks on natural images. Some vision transformers use fixed sinusoidal position embeddings (SPE) for image generation tasks on synthetic images. Some vision transformers use relative position embeddings (RPE) or conditional position embeddings (CPE) for image segmentation tasks on large images.<a href="https://deepganteam.medium.com/vision-transformers-for-computer-vision-9f70418fe41a" data-href="https://deepganteam.medium.com/vision-transformers-for-computer-vision-9f70418fe41a" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">(link)</a> <a href="https://arxiv.org/abs/2102.10882" data-href="https://arxiv.org/abs/2102.10882" class="markup--anchor markup--p-anchor" rel="noopener" target="_blank">(link)</a></p><p name="1e4a" id="1e4a" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Conclusion</strong></p><p name="74b4" id="74b4" class="graf graf--p graf-after--p">In this article, we have learned about how encoding user and historical interaction data are maintained in the concept of Context, and how existing and experimental Attention and Quantization mechanisms can further increase the short-term memory of an LLM. We also learned how positional encoding is used in LLM’s and vision transformers to aid in the maintenance of semantic links within the context. We have seen how positional encoding helps the model to capture the order and position information of the input sequence, how Generative AI applies this to expand context, and how vision transformers apply the transformer architecture and attention to image processing tasks.</p><p name="b2f1" id="b2f1" class="graf graf--p graf-after--p graf--trailing">This is my first article and I hope to continue to create interesting content about the ever expanding world of AI.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@greg.broadhead" class="p-author h-card">Greg Broadhead</a> on <a href="https://medium.com/p/958d7936c42e"><time class="dt-published" datetime="2023-07-24T20:20:29.772Z">July 24, 2023</time></a>.</p><p><a href="https://medium.com/@greg.broadhead/working-with-ai-in-context-958d7936c42e" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 4, 2023.</p></footer></article></body></html>