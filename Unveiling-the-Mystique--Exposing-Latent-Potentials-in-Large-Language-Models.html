<!DOCTYPE html><html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>Unveiling the Mystique: Exposing Latent Potentials in Large Language Models</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">Unveiling the Mystique: Exposing Latent Potentials in Large Language Models</h1>
</header>
<section data-field="subtitle" class="p-summary">
Visualizing the Latent Potential of an LLM
</section>
<section data-field="body" class="e-content">
<section name="e6d0" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="0ceb" id="0ceb" class="graf graf--h3 graf--leading graf--title">Unveiling the Mystique: Exposing Latent Potentials in Large Language Models</h3><figure name="7ffe" id="7ffe" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*wqhmyMio84daQB8LJi1kew.jpeg" data-width="959" data-height="967" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*wqhmyMio84daQB8LJi1kew.jpeg"><figcaption class="imageCaption">Visualizing the Latent Potential of an LLM</figcaption></figure><p name="a10a" id="a10a" class="graf graf--p graf-after--figure">Artificial intelligence, particularly in the realm of natural language processing, has taken a revolutionary leap forward in recent years. Large Language Models (LLMs) such as GPT-4, are exemplifying this advancement by displaying an impressive capacity to understand and generate human-like text. This ability is attributed to the extensive training they undergo, absorbing vast amounts of information from numerous sources.</p><p name="2ef7" id="2ef7" class="graf graf--p graf-after--p">The question we’re diving into today, however, goes beyond the surface-level performance of these models. We’re here to expose and analyze the ‘Latent Potentials’ — the hidden layers and states — within these LLMs giving us insight and understanding of how well-trained a specific model is on a subject for which it might not have undergone direct training.</p><p name="1abc" id="1abc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Anatomy of Latent Potentials</strong></p><p name="952a" id="952a" class="graf graf--p graf-after--p">In a large language model, each word, sentence, and ultimately the context, is processed through numerous hidden layers, also referred to as the ‘Hidden Layers’. These layers are akin to a labyrinth, transforming inputs into complex, multi-dimensional representations. Each layer makes the model more robust, refining the information for more accurate output.</p><p name="596a" id="596a" class="graf graf--p graf-after--p">The hidden states in the model are high-dimensional vectors that represent the model’s internal understanding of each token in the input text. These vectors are the result of complex transformations applied by the model during processing.</p><p name="5903" id="5903" class="graf graf--p graf-after--p">A “bumpy” graph might just mean that the hidden state values for different tokens and/or different dimensions are quite varied. This could be due to the model capturing different features from the input text, while a “flat” graph might mean that the hidden state values are quite similar for different tokens and/or dimensions. This could be due to the model not capturing much variety from the input text.</p><p name="b474" id="b474" class="graf graf--p graf-after--p">The final layer’s hidden states can be seen as the model’s most refined understanding of the input. It represents the culmination of all the transformations applied by the model, and it’s from this final layer that the model makes its predictions (e.g., generating the next word in a sentence for GPT, or determining the class of an input for BERT).</p><p name="e212" id="e212" class="graf graf--p graf-after--p">In the context of transformer models like GPT or BERT, the model consists of several layers, each of which performs a set of operations on the input data. These layers are stacked on top of each other, and the output of one layer is used as the input for the next layer. This is why they are often visualized as a vertical stack or tower, with data flowing from the bottom to the top.</p><p name="1f3f" id="1f3f" class="graf graf--p graf-after--p">Each layer in the model learns to capture different levels of abstraction from the input data. Lower layers (closer to the input) often learn to recognize simple patterns, such as syntax and basic grammar in the case of language models. Higher layers (closer to the output) learn to recognize more complex patterns, such as semantics and context.</p><p name="54c2" id="54c2" class="graf graf--p graf-after--p">When the output is for Layer 31 is plotted, the plot shows the output of the hidden states at the 31st layer of the model when processing the user prompt. Each point in this plot represents the hidden state for a particular token in the prompt at that layer of the model.</p><p name="06d4" id="06d4" class="graf graf--p graf-after--p">By looking at how these hidden state values change from layer to layer for a given prompt, we can get a sense of how the model’s understanding of the prompt evolves as it processes the data.</p><p name="4ff6" id="4ff6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Experiment</strong></p><p name="5b06" id="5b06" class="graf graf--p graf-after--p">To truly gauge how well-trained a model is on a given topic, we propose a unique experiment. We’ll feed the model a series of prompts on a particular subject. The aim is not merely to analyze the output but to probe the hidden layers — transforming their output into heat-maps that reveal the attention each keyword receives.</p><p name="0645" id="0645" class="graf graf--p graf-after--p">Heat-maps and Surface plots provide an excellent medium to visualize the “attention” and hidden layer activations of these models. By comparing the heat-maps from different prompts, we can identify patterns and anomalies, allowing us to derive insights about the model’s level of understanding on the subject at hand.</p><figure name="52f0" id="52f0" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*ZTF_l8GRwxm7xMPl5jBHwQ.png" data-width="2057" data-height="1317" src="https://cdn-images-1.medium.com/max/800/1*ZTF_l8GRwxm7xMPl5jBHwQ.png"><figcaption class="imageCaption">An example of the output layer (Layer 41) of this model’s understanding of the input tokens</figcaption></figure><p name="21cf" id="21cf" class="graf graf--p graf-after--figure">It’s not important to differentiate between negative and positive values for these analysis plots, as the values represent higher dimensional context and understanding which, depending on the transformer architecture can vary in the value range they produce. What is important to notice is the level of ‘bumpiness’ in the output; generally a bumpier plot represents a greater and more nuanced understanding of the user provided text, and the amount of deviation from zero.</p><figure name="70d3" id="70d3" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*purATTw3Ch4APWI992G2Sg.png" data-width="2057" data-height="1317" src="https://cdn-images-1.medium.com/max/800/1*purATTw3Ch4APWI992G2Sg.png"><figcaption class="imageCaption">A heat-map showing the average latent potential activation across all processing layers for each token.</figcaption></figure><p name="1589" id="1589" class="graf graf--p graf-after--figure">Now let’s examine the next plots for a gibberish input message:</p><figure name="e890" id="e890" class="graf graf--figure graf-after--p"><img class="graf-image" data-image-id="1*-CN41ANKdrjwp-H7BSG5Eg.png" data-width="2057" data-height="1317" src="https://cdn-images-1.medium.com/max/800/1*-CN41ANKdrjwp-H7BSG5Eg.png"><figcaption class="imageCaption">An example of the output layer (Layer 41) of this model’s understanding of gibberish input tokens</figcaption></figure><figure name="10dd" id="10dd" class="graf graf--figure graf-after--figure"><img class="graf-image" data-image-id="1*lrcr4RWOAL072A3NBUyHTQ.png" data-width="2057" data-height="1317" src="https://cdn-images-1.medium.com/max/800/1*lrcr4RWOAL072A3NBUyHTQ.png"><figcaption class="imageCaption">A heat-map showing the average latent potential (hidden state) activation across all processing layers for each token.</figcaption></figure><p name="d5a3" id="d5a3" class="graf graf--p graf-after--figure">You can see from the gibberish examples that the model is clearly understanding some combinations of tokens and attempting to make sense of them, by the final processing layer before output (Hidden_Layer 41), the gibberish sample provides a very low level of output in the heat-map and the surface plot is relatively flat in between the spikes, while the more real prompt has values further from zero in the heat-map, indicating a greater understanding of the meaning and nuance of the user message.</p><p name="3050" id="3050" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Breaking Down the Latent Potential</strong></p><p name="d215" id="d215" class="graf graf--p graf-after--p">Let’s consider an example where the chosen subject is “Climate Change”. We provide the model with a variety of prompts, each focusing on different aspects of climate change, from the causes and effects to potential solutions.</p><p name="fab1" id="fab1" class="graf graf--p graf-after--p">By exposing the Latent Potentials and transforming the output into heat-maps, we can see which keywords the model pays attention to. Do “greenhouse gases,” “carbon dioxide,” and “global warming” consistently grab attention? Does the attention shift when we talk about solutions like “renewable energy” or “carbon capture”?</p><p name="273e" id="273e" class="graf graf--p graf-after--p">Furthermore, we can compare the keyword attention across different prompts. Are there any significant differences when we discuss “deforestation” versus “ocean acidification”? By examining these patterns, we can estimate how well the model has been trained on each aspect of the broad subject.</p><p name="f8e7" id="f8e7" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">What These Experiments Mean</strong></p><p name="5de3" id="5de3" class="graf graf--p graf-after--p">Conducting this kind of experiment sheds light on the inner workings of Large Language Models. But why is it important? For one, it can help us refine the training of these models. Understanding their strengths and weaknesses on various subjects can aid in directing more targeted and efficient training efforts.</p><p name="5ddc" id="5ddc" class="graf graf--p graf-after--p">Moreover, these analyses contribute to the broader understanding of AI interpret-ability. By understanding how models process and pay attention to information, we can make strides in the direction of transparent and explainable AI — a field of growing importance as AI models become increasingly integral to our lives.</p><p name="adcd" id="adcd" class="graf graf--p graf-after--p">Visit my GitHub page for the code to generate your own analysis charts!</p><div name="2148" id="2148" class="graf graf--mixtapeEmbed graf-after--p graf--trailing"><a href="https://github.com/KillrBee/HiddenPotentials" data-href="https://github.com/KillrBee/HiddenPotentials" class="markup--anchor markup--mixtapeEmbed-anchor" title="https://github.com/KillrBee/HiddenPotentials"><strong class="markup--strong markup--mixtapeEmbed-strong">GitHub - KillrBee/HiddenPotentials: Analyze the Hidden Potential of LLMs tp perform on un-trained…</strong><br><em class="markup--em markup--mixtapeEmbed-em">Analyze the Hidden Potential of LLMs tp perform on un-trained tasks. - GitHub - KillrBee/HiddenPotentials: Analyze the…</em>github.com</a><a href="https://github.com/KillrBee/HiddenPotentials" class="js-mixtapeImage mixtapeImage u-ignoreBlock" data-media-id="4cf8b25ca73b7fc51b73927b6a663ca5" data-thumbnail-img-id="0*edAdm_UNuQu6ycwL" style="background-image: url(https://cdn-images-1.medium.com/fit/c/160/160/0*edAdm_UNuQu6ycwL);"></a></div></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@greg.broadhead" class="p-author h-card">Greg Broadhead</a> on <a href="https://medium.com/p/5d749734170d"><time class="dt-published" datetime="2023-08-25T17:27:59.920Z">August 25, 2023</time></a>.</p><p><a href="https://medium.com/@greg.broadhead/title-unveiling-the-mystique-exposing-latent-potentials-in-large-language-models-5d749734170d" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 4, 2023.</p></footer></article></body></html>