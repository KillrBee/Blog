<!DOCTYPE html><html><head> {% seo %} <meta http-equiv="Content-Type" content="text/html; charset=utf-8"><title>A Brief Guide To LLM Numbers: Parameter Count vs. Training Size</title><style>
      * {
        font-family: Georgia, Cambria, "Times New Roman", Times, serif;
      }
      html, body {
        margin: 0;
        padding: 0;
      }
      h1 {
        font-size: 50px;
        margin-bottom: 17px;
        color: #333;
      }
      h2 {
        font-size: 24px;
        line-height: 1.6;
        margin: 30px 0 0 0;
        margin-bottom: 18px;
        margin-top: 33px;
        color: #333;
      }
      h3 {
        font-size: 30px;
        margin: 10px 0 20px 0;
        color: #333;
      }
      header {
        width: 640px;
        margin: auto;
      }
      section {
        width: 640px;
        margin: auto;
      }
      section p {
        margin-bottom: 27px;
        font-size: 20px;
        line-height: 1.6;
        color: #333;
      }
      section img {
        max-width: 640px;
      }
      footer {
        padding: 0 20px;
        margin: 50px 0;
        text-align: center;
        font-size: 12px;
      }
      .aspectRatioPlaceholder {
        max-width: auto !important;
        max-height: auto !important;
      }
      .aspectRatioPlaceholder-fill {
        padding-bottom: 0 !important;
      }
      header,
      section[data-field=subtitle],
      section[data-field=description] {
        display: none;
      }
      </style></head><body><article class="h-entry">
<header>
<h1 class="p-name">A Brief Guide To LLM Numbers: Parameter Count vs. Training Size</h1>
</header>
<section data-field="subtitle" class="p-summary">
Ever wondered how AI systems effortlessly understand and spawn natural language texts, regardless of the input and context?
</section>
<section data-field="body" class="e-content">
<section name="1a03" class="section section--body section--first section--last"><div class="section-divider"><hr class="section-divider"></div><div class="section-content"><div class="section-inner sectionLayout--insetColumn"><h3 name="c340" id="c340" class="graf graf--h3 graf--leading graf--title">A Brief Guide To LLM Numbers: Parameter Count vs. Training Size</h3><figure name="1035" id="1035" class="graf graf--figure graf-after--h3"><img class="graf-image" data-image-id="1*B7aEntr78W0aHGLucoQ06A.png" data-width="608" data-height="1080" data-is-featured="true" src="https://cdn-images-1.medium.com/max/800/1*B7aEntr78W0aHGLucoQ06A.png"></figure><p name="a860" id="a860" class="graf graf--p graf-after--figure">Ever wondered how AI systems effortlessly understand and spawn natural language texts, regardless of the input and context? How do they craft answers to your burning questions, draft your emails, or even generate code? The magic wand behind this is the Large Language Models (LLMs) fuelling these systems.</p><p name="3100" id="3100" class="graf graf--p graf-after--p">LLMs are incredibly complex machine learning behemoths that have an innate ability for processing and churning out incredibly human-like text. Their prowess stems from extensive training on a vast corpus of text data gathered across many different domains and trained to identify how patterns within that text create context and meaning.</p><p name="ecb1" id="ecb1" class="graf graf--p graf-after--p">LLM’s are typically defined by parameter count and training size. In this article we will dive into the differences between these numbers and how they influence the capabilities of a model.</p><p name="abcc" id="abcc" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Decoding the Numbers Behind LLMs</strong></p><p name="38d8" id="38d8" class="graf graf--p graf-after--p">In the ever-evolving realm of natural language processing (NLP), you might have stumbled upon acronyms like GPT-4, BLOOM, or LLaMA, often followed by intriguing numbers such as 175B, 60B, down to 7B. These aren’t just random combinations of numbers and letters; they refer to the size and often, the capability of Transformer based LLMs.</p><p name="55c7" id="55c7" class="graf graf--p graf-after--p">LLMs are the current powerhouse of artificial intelligence (AI) systems that almost magically comprehend and craft natural language text.</p><p name="7df3" id="7df3" class="graf graf--p graf-after--p">But what story do these numbers tell, and why should we care? Let’s dive and unravel what these figures symbolize, their link to the tokens used during training, and their influence on an LLM’s vocabulary size, creativity and raw capabilities.</p><p name="5135" id="5135" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Understanding the Numbers Behind LLMs</strong></p><p name="eca7" id="eca7" class="graf graf--p graf-after--p">The numbers trailing the names of open-source LLMs denote the model’s parameters. Think of parameters as the conductors orchestrating how the model manipulates and understands the input data and produces outputs. They could manifest as weights or biases, influencing the significance of specific input features on the generated output.</p><p name="fdd3" id="fdd3" class="graf graf--p graf-after--p">A larger parameter count generally equates to a model with increased complexity and adaptability (although not strictly true across different architectures, generally true within a transformer architecture). A large language model with a higher parameter count can discern more intricate patterns from the data, paving the way for richer and more precise outputs. But, as with many things in life, there’s a trade-off. A surge in parameters means higher computational demands, greater memory needs, and a looming risk of over-fitting.</p><p name="e2ad" id="e2ad" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Training Data, Parameters, and Variations Across Models</strong></p><p name="5377" id="5377" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Training Ground for LLMs</strong></p><p name="154b" id="154b" class="graf graf--p graf-after--p">The most common and recent Large Language Models owe their adeptness to vast libraries of text data, primarily hailing from the Internet. By predicting the subsequent word or token from their training, they generate lifelike language patterns and linguistic intricacies. The foundational knowledge gained from this training data allows them to be further fine-tuned towards specialized tasks or domains, transforming them into chatbots, summarizers, translators, or even coders.</p><p name="0510" id="0510" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Deciphering LLM Parameters</strong></p><p name="fee0" id="fee0" class="graf graf--p graf-after--p">But what defines the proficiency and versatility of LLMs? The heart of the matter lies in the parameters of LLMs which establish a blueprint the model’s architecture and base capabilities. LLM parameters are the processing guideposts that establish the model’s transformation of input data to output. These parameters, whether weights or biases, govern the impact of specific input features on the resultant output.</p><p name="53f6" id="53f6" class="graf graf--p graf-after--p">The complexity and expressiveness of an LLM increases with an increase in parameters. While this empowers the model to discern a broader spectrum of patterns, it’s a double-edged sword. The flip-side? increased computational demands, an increase in memory requirements, and the ever-present spectre of model overfitting.</p><p name="5fdd" id="5fdd" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Many Faces of LLM Parameters</strong></p><p name="fac7" id="fac7" class="graf graf--p graf-after--p">LLM parameters aren’t a one-size-fits-all. They morph across models, influenced by the model’s size, architectural blueprint, and the data that shaped it. The model’s magnitude is often gauged by its parameter count. For instance, OpenAI’s GPT-3 boasts nearly 175 billion parameters (nearly 45 Terrabytes of raw text data), BLOOM stands tall with 176 billion parameters, and Meta’s LLaMA offers the choice of four sizes: 7B, 13B, 33B, and 65B parameters.</p><p name="c677" id="c677" class="graf graf--p graf-after--p">While the majority of LLMs find their roots in the Transformer architecture, replete with layers of attention and feed-forward networks, their individuality shines in the specific attention mechanisms they harness, whether it’s sparse attention, global-local attention, or nuanced self-attention.</p><p name="8282" id="8282" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Training Data and Influence of Parameters: Tokens</strong></p><p name="34c5" id="34c5" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Data Behind the Model</strong></p><p name="3ab9" id="3ab9" class="graf graf--p graf-after--p">The training data for a model isn’t merely about quantity but also quality and diversity. While some models, like GPT-4, are nurtured on expansive datasets enriched with human feedback and adversarial testing, others like BLOOM are groomed on meticulously curated datasets that deliberately omit high-toxicity sequences. Yet others, like LLaMA, benefit from a composite of public data infused with top-tier annotations.</p><p name="ab7e" id="ab7e" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Ripple Effect of Parameters on Input and Output</strong></p><p name="b7d3" id="b7d3" class="graf graf--p graf-after--p">The parameters of an LLM are instrumental in shaping its input and output dynamics, encompassing aspects of quality, diversity, and reliability. Here’s a breakdown:</p><ul class="postList"><li name="e904" id="e904" class="graf graf--li graf-after--p"><strong class="markup--strong markup--li-strong">Quality</strong>: Refers to the precision, relevance, and coherence of the generated content.</li><li name="18d2" id="18d2" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Diversity</strong>: Speaks to the variety, originality, and creativity of the content.</li><li name="eb78" id="eb78" class="graf graf--li graf-after--li"><strong class="markup--strong markup--li-strong">Reliability</strong>: Addresses the consistency, robustness, and accuracy of the content.</li></ul><p name="655f" id="655f" class="graf graf--p graf-after--li">In general, models with high parameter count will produce content of superior quality and diversity. Their vast parameter set equips them to learn and predict more comprehensively. Yet, it’s essential to understand that more isn’t always better. In specific contexts, a leaner model, optimized for a particular domain, might outperform its bulkier general knowledge counterpart.</p><p name="8680" id="8680" class="graf graf--p graf-after--p">Similarly, models with a richer parameter tapestry can better navigate diverse and intricate inputs. Their large parameter set grants them the ability to recognize an array of tokens and features. Yet again, it’s not a universal rule. At times, a model with lesser parameters but fine-tuned to resonate with human values might be the underdog, surpassing its bigger peers.</p><p name="6bcd" id="6bcd" class="graf graf--p graf-after--p">Ultimately the parameters determine the weight, or influence that specific tokens have on creating semantic links, context and probabilities used to generate the internal representation of the user input on the resulting output.</p><p name="9fc6" id="9fc6" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Vocabulary Size and a Deep Dive into Parameters and Training data</strong></p><p name="9bb4" id="9bb4" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Parameters and Tokens: Unravelling the Connection</strong></p><p name="ae49" id="ae49" class="graf graf--p graf-after--p">While parameters offer a glimpse into an LLM’s complexity, tokens provide a window into its breadth of knowledge. Contrary to some beliefs, these two aren’t directly linked. Tokens, which can range from words, subwords, characters to symbols, signify the chunks of text the model processes. The sheer number of tokens a model trains on stands testament to its exposure — more tokens mean a more worldly-wise model.</p><p name="acf5" id="acf5" class="graf graf--p graf-after--p">However, a huge token count isn’t a silver bullet. It drags along challenges like protracted data collection, ethical conundrums, and a plateauing performance curve.</p><p name="9342" id="9342" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">The Vocabulary Size: A Linguistic Toolbox</strong></p><p name="c1ba" id="c1ba" class="graf graf--p graf-after--p">The vocabulary of an LLM serves as its linguistic toolbox — the set of unique tokens it recognizes and wields. A model’s vocabulary richness often hinges on its parameters, weight and biases, and token training count. Although more parameters and tokens generally produce a more extensive vocabulary, this expansion isn’t without its pitfalls, such as increased storage demands and computational costs.</p><p name="433b" id="433b" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Inside the World of LLM Parameters</strong></p><p name="a97a" id="a97a" class="graf graf--p graf-after--p">Parameters, such as attention weights and embedding vectors, are the unsung heroes behind an LLM’s prowess. The attention mechanism, for instance, empowers a model to selectively zoom into pivotal segments of input, sidelining the extraneous bits. An attention weight, a crucial parameter, dictates this selective focus. In the realm of translations or summaries, these weights become invaluable.</p><p name="3b37" id="3b37" class="graf graf--p graf-after--p">Embedding vectors, another set of parameters, transmute textual tokens into numerical avatars, encapsulating their essence. These correlated chunks of information can be placed directly within a model, or can exist in an external data source known as a Vector DB. For example, when a model reads a PDF from the internet or a local file, it is first using machine learning to extract the information as text through Optical Character Recognition (which is a type of machine vision), then taking that text and converting it into smaller chunks of data which is then placed into a vector database that can be searched against and utilized as knowledge when a model is asked a question. Another example is in language translating, these embedding vectors bridge the linguistic gap, encoding tokens from the source language and decoding them in the target language.</p><p name="a563" id="a563" class="graf graf--p graf-after--p"><strong class="markup--strong markup--p-strong">Wrapping Up the World of LLMs</strong></p><p name="ccbf" id="ccbf" class="graf graf--p graf-after--p">The mystique behind the numbers trailing open-source LLMs, such as GPT-4, BLOOM, or LLaMA, is richly layered. These numbers, indicative of parameters, cast light on a model’s complexity and expressive capabilities. While they might dwarf other entities, like smartphones or Wikipedia articles, in their magnitude, they aren’t the zenith in the AI model universe.</p><p name="884b" id="884b" class="graf graf--p graf-after--p">The parameters of an LLM are the linchpins, shaping its ability to process and regurgitate natural language text in diverse contexts. Whether it’s the attention weights that determine the model’s focus or the embedding vectors that translate tokens into meaningful numerical representations, these parameters are the cogs in the vast machinery of LLMs.</p><p name="1c8c" id="1c8c" class="graf graf--p graf-after--p">The number of tokens used to train the base model define the inherent knowledge a model possesses, whether that be its knowledge of medicine, or knowledge of the linguistic style that a person associated with a corpus of text; which is how you can get an LLM to respond in the style of a famous person, like Sir Michael Caine.</p><p name="19e0" id="19e0" class="graf graf--p graf-after--p graf--trailing">In the grand scheme of AI and NLP, understanding these numbers and their significance is akin to possessing the Rosetta Stone, offering insights into the intricate world of Large Language Models and their differing capabilities.</p></div></div></section>
</section>
<footer><p>By <a href="https://medium.com/@greg.broadhead" class="p-author h-card">Greg Broadhead</a> on <a href="https://medium.com/p/894a81c9258"><time class="dt-published" datetime="2023-08-25T21:56:36.752Z">August 25, 2023</time></a>.</p><p><a href="https://medium.com/@greg.broadhead/a-brief-guide-to-llm-numbers-parameter-count-vs-training-size-894a81c9258" class="p-canonical">Canonical link</a></p><p>Exported from <a href="https://medium.com">Medium</a> on October 4, 2023.</p></footer></article></body></html>